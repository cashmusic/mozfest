
![image](http://www.itp.jasonsigal.cc/wp-content/uploads/2014/10/2014-10-26-12.03.43.jpg)

A few of the ideas that came out of this session:
- frequency analysis --> generative ASCII art
- pre-analyze music, save performance for rendering
- trigger events with JSON
- visualize music then turn the visualization back into music
- waypoints in song change visualization style
- map particle system to text / lyrics
- new mobile interfaces
- adaptive phone background / lighting changes based on music around you
- translate closed captions to time data
- interactive web-wide jam session
- scrape lyrics - text renderer
- visual ways to display sound, beyond just peaks
- map visuals to stems
- Synesthesia
- new visualization styles
- live generative visuals
- find natural "break points" in spoken audio

With all of these incredible ideas, a two hour session was barely enough time to even begin. But so far, a few participants have shared some work that came out of this session, and we hope to hear more sooon!
- [lyrics visualizer](http://therewasaguy.github.io/mozfest/output/lyrics_visualizer/) by @indefinit
- [recorder](http://markmoriarty.com/mozfest/) by @markitics 
